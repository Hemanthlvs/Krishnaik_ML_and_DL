# Positional Encoding in Transformers

*   **What is Positional Encoding?**
    *   It's a way to **represent the order of a sequence** of words or tokens.
    *   It ensures the Transformer understands which word comes first, second, and so on in a sentence.

*   **Why is it Needed for Transformers?**
    *   **Transformer Advantage**: Transformers can process all words (tokens) in a sentence **in parallel**. This is a major advantage for speed.
    *   **Transformer Drawback**: Because of parallel processing, the self-attention layer **lacks the sequential structure** or order of the words.
    *   **The Problem**: Without positional encoding, the Transformer cannot tell if "lion eats tiger" or "tiger eats lion" are different, as all words are processed at once. Both sentences might get the **same vector representation**, even though their meanings are completely different due to word order.

*   **How Positional Encoding Works (General Idea)**
    *   A **positional encoded vector** is created for each word.
    *   This positional encoded vector is then **added to the original word's embedding vector**.
    *   The combined (original embedding + positional encoding) vector is then fed into the self-attention layer. This allows the Transformer to understand the word's position.

*   **Initial Thought (and why it doesn't work well)**
    *   One simple idea is to add a new dimension to the word vector and assign a number for its position (e.g., 1 for the first word, 2 for the second).
    *   **Problem**: This works for short texts, but for long texts (like a book with over a lakh words), the position numbers become **unboundedly large**.
    *   Large numbers can cause problems during the **backpropagation** process when updating model weights.

*   **Types of Positional Encoding**
    1.  **Sinusoidal Positional Encoding**
    2.  **Learned Positional Encoding**

*   **Sinusoidal Positional Encoding (Detailed)**
    *   This is the technique mentioned in the "Attention Is All You Need" research paper.
    *   **How it works**: It uses **sine and cosine functions of different frequencies** to create the positional encodings.
    *   **Advantage**: The values generated by sinusoidal encoding always range **between -1 and +1**, solving the unbounded number problem.
    *   **Formulas**:
        *   For even dimensions (`2i`): `PE(pos, 2i) = sin(pos / 10000^(2i/D_model))`
        *   For odd dimensions (`2i+1`): `PE(pos, 2i+1) = cos(pos / 10000^(2i/D_model))`
    *   **Breakdown of terms**:
        *   `pos`: The **position** of the word in the sequence (e.g., 0 for the first word, 1 for the second).
        *   `i`: The **dimension** within the positional encoding vector.
        *   `D_model`: The **dimensionality of the word embeddings** (e.g., if a word vector has 4 values, `D_model` is 4).
		
		*	Dimension indices go like this:
			- Dimension 0 → \(2i = 0\), so \(i = 0\)
			- Dimension 1 → \(2i + 1 = 1\), so \(i = 0\)
			- Dimension 2 → \(2i = 2\), so \(i = 1\)
			- Dimension 3 → \(2i + 1 = 3\), so \(i = 1\)		
		
    *   **Why both Sine and Cosine?**: If only sine functions were used, it's possible that two different positions could produce the **same sine value**, causing the model to miss the true order of elements. By combining sine and cosine functions, the positional encodings are made unique enough to ensure the order is maintained.
    *   **Example Calculation**: For a word at position 0 in a 4-dimensional embedding (`D_model=4`):
        *   `PE(0, 0)` (using sine): `sin(0 / 10000^(0/4)) = sin(0) = 0`
        *   `PE(0, 1)` (using cosine): `cos(0 / 10000^(0/4)) = cos(0) = 1`
        *   `PE(0, 2)` (using sine): `sin(0 / 10000^(2/4)) = sin(0) = 0`
        *   `PE(0, 3)` (using cosine): `cos(0 / 10000^(2/4)) = cos(0) = 1`
        *   So, for the first word (position 0), the positional encoding would be ``.
    *   This process is repeated for every word in the sequence (e.g., position 1 for the second word, position 2 for the third, etc.).

*   **Learned Positional Encoding**
    *   In this method, the **positional encodings are learned** during the training process of the model.
    *   This means they are updated through backpropagation.(we can ignore this as of now)

*   **Final Output**
    *   After adding the positional encoding to the word embedding, the combined vector for each word (e.g., for "cat": `[0.5, 0.6, 0.7, 0.8]` + `[0.84, 0.54, 0.01, 0.99]`) is passed to the self-attention layer.
    *   This ensures that the self-attention mechanism, and subsequent layers like the feedforward neural network, have the necessary information about the **order and context** of the words to produce meaningful contextual vectors (`Z1`, `Z2`, etc.).
    *   This is a crucial difference from Recurrent Neural Networks (RNNs), which inherently handle order by processing words one at a time.